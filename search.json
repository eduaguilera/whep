[{"path":"https://eduaguilera.github.io/whep/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 whep authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"cloning-the-repository","dir":"Articles","previous_headings":"Git intro","what":"Cloning the repository","title":"Follow the workflow","text":"get started Git, need operating system recognize Git commands. assume Windows, install Git . know whether 32 64 bits version, likely need 64 bit one. now something called ‘Git Bash’ installed, like command line tool (similar Windows CMD). can open Git Bash inside specific directory (just technical name folders use now ) right-clicking desired directory file explorer selecting ‘Open Git Bash ’. However, recommend learn basic commands navigate command line (now , writing <-text> part command, just use placeholder need write ): Print current directory: just useful can see right now. List files current directory: Suppose know exact path follow know inside certain subdirectory one right now. Listing everything current directory ls useful way spot subdirectory looking , can navigate inside cd. Move another directory relative one right now: can use ls cd <relative-path> repeatedly directory want place subdirectory containing repository. , can double check using pwd. assume repository want contribute already exists. can go page GitHub copy URL seen image :  git terminology used ‘downloading’ repository local file system ‘cloning’. can clone remote repository (case GitHub) using following command: called cloning via HTTPS. browser open ask introduce GitHub credentials. ways cloning like SSH, scope guide.","code":"pwd ls cd <relative-path-where-to-move> git clone <url-you-copied>"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"pulling-remote-changes","dir":"Articles","previous_headings":"Git intro","what":"Pulling remote changes","title":"Follow the workflow","text":"Now new directory created content repository local file system. now see basic git commands need daily usage. assume inside repository. explain example. Suppose want start contributing repository. good practice (one enforce use) make code changes ‘different place’ ones currently see repository. things see now called ‘main branch’, make code changes ‘new branch’, start content main one, evolve different changes. done anything yet, main branch (maybe called ‘main’ ‘master’, just conventions, assume called ‘main’). can use command git status check (mind terminal looks different screenshots, can use commands Git Bash):  local version repository need match remote version (one store GitHub case), start work new branch, keep main branch date case someone added new code GitHub repository since last time checked. get new remote changes local repository using command  case already remote changes, message says ‘Already date’, message different missing changes. ‘easy way’ . command git pull tries fetch changes equivalent remote branch, .e., one name remote local repository. may always work expected way always specify remote branch want get changes (highly recommend always using explicitly): example, imagine asked someone help branch added new changes branch, locally. , branch called -branch, already branch locally, want use command Likewise, first example shown (keeping main branch updated), always explicit:","code":"git pull git pull origin <name-of-remote-branch> git pull origin my-branch git pull origin main"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"creating-our-own-branch","dir":"Articles","previous_headings":"Git intro","what":"Creating our own branch","title":"Follow the workflow","text":"pull, now safely date remote changes. Now time learn create ‘branch’, start working new code. use following command:  command git checkout <name--branch> used change one branch another (now see files changes branch). Additionally, add -b option, create branch given name already exist, case example. branch name something like author/name--branch. Thus, common practices naming branches (follow) : contain caps (lowercase) Words separated dashes (-) name includes author descriptive name separated slash (/) descriptive name ideally start action (verb) imperative style (fix, create, test…). Ermenegildo wants create code preprocessing bilateral trade data, acceptable branch name ermenegildo/preprocess-bilateral-trade-data.","code":"git checkout -b <name-of-branch>"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"adding-changes-to-our-branch","dir":"Articles","previous_headings":"Git intro","what":"Adding changes to our branch","title":"Follow the workflow","text":"Now branch can start working changes. work , keep track changes git. can add changes using command dot means ‘directory’, essentially adds new changes, .e. things inside directory. can add just specific file instead using command  adding changes, must ‘commit’ . commit step actually saves changes git history. command common practice commit messages start verb infinitive (imperative style), indicating action performed, e.g., 'Create tests bilateral trade data preprocessing'.  common practice make small commits, , include just changes commit, easier keep track work’s history, instead just single commit done everything. Ultimately, amount commits decision, just one commit per branch.","code":"git add . git add <relative-name-of-file> git commit -m 'Some descriptive message for your changes'"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"pushing-our-changes","dir":"Articles","previous_headings":"Git intro","what":"Pushing our changes","title":"Follow the workflow","text":"committing, now changes local git history, probably also add remote GitHub repository. using command Now able see changes branch GitHub , just need select branch instead main one. remember push changes regularly remote repository. Otherwise risk bunch code features local computer lost something happened . aligned previous suggestion creating many smaller commits opposed giant ones, can also push frequently.","code":"git push origin <name-of-branch>"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"creating-a-pull-request","dir":"Articles","previous_headings":"Git intro","what":"Creating a pull request","title":"Follow the workflow","text":"Suppose done changes want add main branch. Mixing one branch another known ‘merging’. case like merge new branch main branch. can done forcefully, common practice following create known ‘Pull request’ branch main one, directly GitHub, pushed changes.   can see changes made (differ main branch) clicking ‘Create pull request’. see following, add title description explain done. finally click ‘Create pull request’ .  Now Pull Request (often abbreviated PR) created next step ask someone’s review.  Ideally changes merged someone else reviews code. person might find things change request changes merging, keep working branch satisfied. accept changes ready merge branch main one, process done. However, sometimes additional step must passed merging, related automatic code checks, e.g. check whether code well formatted whether passes tests successfully. configured, can run automatically creating Pull Request. indeed work , explain automatic checks better Automatic checks Pull Requests section. working branch, others may merged branches main branch branch outdated. creating Pull Request , make sure branch also date everything already main branch. Recall pulling remote changes section can command Even locally branch directly try fetch changes different remote one (case main), works expected, , tries merge new changes main branch local one. automatic merge works times, sometimes may find conflicts, program know combine everything neatly. happens, must manually check parts code kept. next section explain solve conflicts.","code":"git pull origin main"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"solving-conflicts","dir":"Articles","previous_headings":"Git intro","what":"Solving conflicts","title":"Follow the workflow","text":"noted previous section, sometimes git pull another branch remote one (missing changes), can find conflicts. conflict looks like : conflict, least three lines added git separate conflicting parts. conflict starts line <<<<<<< HEAD, get ======= line, lines content added. one end >>>>>>> some_branch_name, lines content someone else added yet. solving conflict essentially means removing three lines added git. three options . decide one want depending situation: Keep content. Solving conflict involve removing lines except: Keep content. remove everything except: Keep () content parts, even adapt adding things. remove three lines added git everything else want keep, leaving something like mix: find conflicts (advise manually), use text finding tool editor, look text HEAD, always appears first line conflict. solved conflicts, rest steps explained previous sections, involving git add git commit, pull also counts code change, make commit . case wondering, perform pull without conflicts, creating commit git , automatically. whether solved conflicts git , always commit representing .","code":"<<<<<<< HEAD this is my content that I just added ======= this is some different conflicting content from the branch I pulled from >>>>>>> some_branch_name this is my content that I just added this is some different conflicting content from the branch I pulled from this is my content that I just added this is some different conflicting content"},{"path":[]},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"project-structure","dir":"Articles","previous_headings":"R package and renv intro","what":"Project structure","title":"Follow the workflow","text":"seems clear even though work fine bare R scripts run directly, working large project makes sense kind file structure, keep everything organized. can build ad-hoc file structures, probably come something rather simple. , instead, focus using standard structure R package. standard everyone follow want projects turn packages can publicly downloaded anyone CRAN repositories. Just way , e.g., install.packages(tidyverse) install Tidyverse packages, follow standard R package structure, can upload package one install.packages(your_package) way. Even want upload package, still advantages follow structure. one follow, rest section try explain different parts, become part workflow. whole structure R package:  Luckily, lot files need know , least now, try explain important ones next sections. whole R packages book followed setup basics project. well written available free online, interested knowing R packages project structure, recommend checking book.","code":""},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"virtual-environments-with-renv","dir":"Articles","previous_headings":"R package and renv intro","what":"Virtual environments with renv","title":"Follow the workflow","text":"just mentioned going use R package structure, seems R package developers use renv… ? least seem include renv related files package repositories… Well, use ? writing guide bit confused mixing things, conclusion just hurt way, renv just makes things easier without apparent drawbacks (tell know ). creating packages, want make sure work fresh installations, .e., computers anything unnecessary installed. package creation process use , need know anything renv, fine. packages use file called DESCRIPTION includes information packages needs dependencies, see later . can just try benefit using virtual environments. OK, virtual environments? fancy term, practical meaning quite simple. First consider following: using , means just global R installation computer, whenever install package, installed globally. want run someone’s code use bunch packages usually , install able run code, mix packages. want uninstall , lot manual work make sure know (package dependencies also installed, sure used packages also package already ). want write code uses packages, want another person run , make list packages used project, install packages projects necessary . even make ‘package list’, person go whole code run install new package every time code fails missing one. Overall, poor experience. Virtual environments try fix . Essentially, provide ‘local’ installation packages, visible inside project, get mixed global R installation individual projects. practice, virtual environment just folder containing installed packages, isolated folder contains global R installation. like several different R installations, one packages versions. Chances follow guide existing repository already using renv (can skip renv::init() step). case, open R prompt root directory project run inside prompt: probably ask close reopen clean prompt. , every time open R prompt inside project, automatically use renv work within virtual environment. use renv first time project already uses , open R prompt root directory, renv package installed automatically. Now renv, can, example, install testing package install.packages(\"testthat\") global installation, means work inside project. way isolating project dependencies making projects reproducible, letting others know exactly packages code needs run, add unnecessary ones may projects, mentioned previously. ‘list’ required packages project, along versions, used renv manage virtual environment, file called renv.lock. installing new packages, file updated automatically manually running update renv.lock file packages renv finds used code. reason need install package explicitly used code, may fail recognize . case, instead explicitly call renv::snapshot(type=\"\") force every package renv environment added renv.lock. push file repository. someone else wants reproduce code, may run install packages renv.lock may still installed, , project level, conflicting global R installation. use GitHub others, might also need every time pull remote changes someone else included new package, date . case, opening R shell, probably remind missing packages virtual environment message:  basically need start using virtual environment, keeping mind commands renv::snapshot(): add new required packages renv.lock file renv::restore(): install packages renv.lock yet wrote introduction renv reading package documentation. want learn , can read package website. directly related renv usage, wanted highlight Windows may errors trying install R packages. times may related missing operating system dependencies commands. Windows easily fixable installing version Rtools matches R version. selecting version can download clicking first installer link. installing Rtools, can try install R packages wanted.","code":"renv::init() renv::snapshot() renv::restore()"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"writing-code","dir":"Articles","previous_headings":"R package and renv intro","what":"Writing code","title":"Follow the workflow","text":"Looking back package’s file structure, R/ directory put main code. R files stored must contain top-level code, , must inside functions. can add one function file somehow related, must many either. file becomes large several functions inside, consider splitting shorter files. Take following code example, written colleague Justin (understand code, can keep reading). save R/sources.R. sample code things keep mind: code written inside functions, three . name two starts dot. convention private functions. Private functions just helpers used functions file, need used outside. functions private, called public, ones want ‘export’, sense want allow used outside file. sources.R example, first function public. public function large commented section , line starting #'. special type comment considered documentation. Every public function must documented way (special function documentation next section). private functions can introduced explanatory comments consider necessary, normal comments instead (starting just #, without single quote). important take anyway files contain code inside functions nothing outside .","code":"#' Create a new dataframe where each row has a year range into one where each #' row is a single year, effectively 'expanding' the whole year range #' #' @param trade_sources A tibble dataframe #' where each row contains the year range #' #' @returns A tibble dataframe where each row #' corresponds to a single year for a given source #' #' @export #' #' @examples #' trade_sources <- tibble::tibble( #'   Name = c(\"a\", \"b\", \"c\"), #'   Trade = c(\"t1\", \"t2\", \"t3\"), #'   Info_Format = c(\"year\", \"partial_series\", \"year\"), #'   Timeline_Start = c(1, 1, 2), #'   Timeline_End = c(3, 4, 5), #'   Timeline_Freq = c(1, 1, 2), #'   `Imp/Exp` = \"Imp\", #'   SACO_link = NA, #' ) #' expand_trade_sources(trade_sources) expand_trade_sources <- function(trade_sources) {   non_na_cols <- c(\"Trade\", \"Timeline_Start\", \"Timeline_End\", \"Timeline_Freq\")   trade_sources |>     dplyr::filter(!.any_na_col(non_na_cols)) |>     .expand_trade_years() |>     dplyr::mutate(       Name = dplyr::if_else(         Info_Format == \"year\", paste(Name, Year, sep = \"_\"), Name       ),       ImpExp = `Imp/Exp`,       In_Saco = as.integer(!is.na(SACO_link)),     ) }  .expand_trade_years <- function(trade_sources) {   trade_sources <- dplyr::mutate(trade_sources, No = dplyr::row_number())    trade_sources |>     dplyr::group_by(No) |>     tidyr::expand(Year = seq(Timeline_Start, Timeline_End, Timeline_Freq)) |>     dplyr::inner_join(trade_sources, by = \"No\") }  .any_na_col <- function(cols_to_check) {   dplyr::if_any(dplyr::all_of(cols_to_check), is.na) }"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"function-documentation","dir":"Articles","previous_headings":"R package and renv intro","what":"Function documentation","title":"Follow the workflow","text":"special commented section seen previous example used package called roxygen2. follow exact syntax, package can automatically build really neat documentation package us. Let’s try understand basic structure. reference, different parts: small description function, nothing else. small description parameter function receives. like: see think OK add line breaks , long parameter starts @param. small description value function returns. start @returns. simple line containing @export indicate function can used package, .e., public. ‘code’ section examples illustrate function’s behavior. must start @examples, can write usual R code. processed, automatically runs code adds lines output documentation. options enough get us started nice documentation. Writing articles section learn generate see documentation. example, look something like (note autogenerated example code output):","code":"#' Create a new dataframe where each row has a year range into one where each #' row is a single year, effectively 'expanding' the whole year range #' @param param_name_1 Description of param 1 #' @param param_name_2 Description of param 2 #' ... #' @param trade_sources A tibble dataframe #' where each row contains the year range #' @returns A tibble dataframe where each row #' corresponds to a single year for a given source #' @export #' @examples #' trade_sources <- tibble::tibble( #'   Name = c(\"a\", \"b\", \"c\"), #'   Trade = c(\"t1\", \"t2\", \"t3\"), #'   Info_Format = c(\"year\", \"partial_series\", \"year\"), #'   Timeline_Start = c(1, 1, 2), #'   Timeline_End = c(3, 4, 5), #'   Timeline_Freq = c(1, 1, 2), #'   `Imp/Exp` = \"Imp\", #'   SACO_link = NA, #' ) #' expand_trade_sources(trade_sources)"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"package-data","dir":"Articles","previous_headings":"R package and renv intro","what":"Package data","title":"Follow the workflow","text":"data work might large. final datasets want produce large, can’t directly include package size limits recommendations. case, export functions. way, package won’t contain output dataset, user generate running function, stored computer. example function get_wide_cbs(). Unless generate dataset function, also probably using large input dataset just process somehow. reading large datasets, see relevant Reading large files section. now , section assume small datasets (maybe couple megabytes) inputs. go public private ones. public ones want export users package, private ones intended used code. Let’s start exported datasets. whole aim allow users (even code) access easily writing my_pkg::my_dataset. order achieve , must follow steps: Create file data-raw/my_dataset.R. Scripts inside data-raw folder aren’t included part package. just helpers us generate actual data. Inside file processing , assuming variable called my_dataset, end script calling usethis::use_data(my_dataset). automatically create .rda file data/my_dataset.rda. manually run script. , can now refer dataset my_pkg::my_dataset. can directly create data script data-raw/my_dataset.R, can make rather short just importing data another raw file. case, recommend raw file CSV inst/extdata folder, say, inst/extdata/my_raw_dataset.csv. accessibility, everyone can see data comes regardless whether know read .rda file . data-raw/my_dataset.R script look like: Every time introduce change raw CSV file, run script . overwrite = TRUE exactly purpose, my_dataset.rda file overwritten updated data. Document dataset. previous section learned document functions. Datasets aren’t functions, ’re documented similarly. start creating file R/my_dataset.R. Note name matches data-raw/my_dataset.R. doesn’t need match variable used usethis::use_data(), match . can define one dataset file think ’re related, can also use general name file. document dataset, also using roxygen2 comments: can see, use roxygen2 style comments right line containing character vector name dataset, case \"my_dataset\". Now dataset correctly documented devtools::document() pkgdown::build_site()/pkgdown::build_reference(). Now talk internal data. data developers package use throughout code. either actual tibble datasets just bare constants. value doesn’t change like share throughout whole package code applies . Creating internal data quite similar exported data: Create file data-raw/constants.R doesn’t already exist. internal data, defined file. file look like : can see, can pass one variable usethis::use_data(). include constants call function. addition, must also include internal = TRUE option identify internal data. Manually run previous file. create single file R/sysdata.rda, contains internal data. can now refer data way exported data, e.g., my_pkg::my_constant_tibble my_pkg::my_constant_number, available package’s code, won’t exported package users. , time want add new internal data modify existing entries, manually run script . Whether data worth exported part package just used internal, decision, now know implement . section heavily inspired Data chapter R Packages book, recommend reading want dive deeper.","code":"my_dataset <- here::here(\"inst\", \"extdata\", \"my_raw_dataset.csv\") |>   readr::read_csv()  usethis::use_data(my_raw_dataset, overwrite = TRUE) #' Title of my dataset #' #' My description of my dataset #' #' @format #' What my dataset is. I would ideally make it a tibble and explain all #' columns. My dataset contains the following columns: #' - `column_1`: My explanation of column 1. #' - `column_2`: My explanation of column 2. #' - `column_3`: My explanation of column 3. #' #' @source Where my data comes from. Maybe an external link if you have one. \"my_dataset\" my_constant_number <- 0.65 my_constant_name <- \"name\" my_constant_tibble <- tibble::tribble(   ~col_1, ~col_2,   1,      2,   3,      4 )  usethis::use_data(   my_constant_number,   my_constant_name,   my_constant_tibble,   internal = TRUE,   overwrite = TRUE )"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"writing-tests","dir":"Articles","previous_headings":"R package and renv intro","what":"Writing tests","title":"Follow the workflow","text":"just wrote function, done , now move another function… . probably thought check somehow function indeed correct expect. Right now easy just load function R prompt try examples , next month someone make change code? manual testing make sure break functionality. need change dozens functions? much time spend testing ? think can understand really time consuming better way. Tests can automatized. can write tests whenever create new function, together prove function expect, later add changes function, already test can run automatically see function still correct. course, completely accurate. Maybe changed function, functionality also changed, test accurate anymore tweaked well, represent really want. still much less work always testing function manually R prompt, eventually just get used . package used write tests well integrated R package creation workflow testthat. using write automated tests. , looking structure R package, tests go (surprise!) directory tests/. directory file called testthat.R setups testthat changed, actual tests write go tests/testthat/ subdirectory. convention name test files way R files test- prefix. case, example, R file R/sources.R, test file tests/testthat/test-sources.R. Let’s see one tests look like: , understand whole code. Just note use two functions testthat package: testthat::test_that: main function used delimit test . receives text description test checking, body containing code test . testthat::expect_equal: just one many utilities testthat brings actually assert things test’s code. probably general assert, just checks everything identical arguments, including internal object metadata, just “appearance” (may see printing object). can look testing utility functions documentation. now test. execute ? recommended run test usual R code (e.g. run file script). Instead, functions provided testthat running tests. : testthat::auto_test_package(): one run tests package first time, stop running, wait code changes. means whenever ‘save’ test file, reruns tests file. extremely useful actively writing tests, can get fast feedback. testthat::test_file(): one receives argument path test file, runs tests inside . example, run case testthat::test_file(\"tests/testthat/test-sources.R\"). testthat::test_dir(): case, different running tests e.g. subdirectories tests/testthat one. subdirectory tests/testthat/sources many test files related sources, run testthat::test_dir(\"tests/testthat/sources\") test files inside directory executed. testthat::test_package(): general one. just runs tests project. can useful run tests actively working . supposed make tests pass, see next section, checks package must pass valid (can publicly uploaded), tests definitely one .","code":"library(\"testthat\")  test_that(\"trade source data is expanded from year range to single year rows\", {   trade_sources <- tibble::tibble(     Name = c(\"a\", \"b\", \"c\", \"d\", \"e\"),     Trade = c(\"t1\", \"t2\", \"t3\", NA, \"t5\"),     Info_Format = c(\"year\", \"partial_series\", \"year\", \"year\", \"year\"),     Timeline_Start = c(1, 1, 2, 1, 3),     Timeline_End = c(3, 4, 5, 1, 2),     Timeline_Freq = c(1, 1, 2, 1, NA),     `Imp/Exp` = \"Imp\",     SACO_link = NA,   )   expected <- tibble::tibble(     Name = c(\"a_1\", \"a_2\", \"a_3\", \"b\", \"b\", \"b\", \"b\", \"c_2\", \"c_4\"),     Trade = c(\"t1\", \"t1\", \"t1\", \"t2\", \"t2\", \"t2\", \"t2\", \"t3\", \"t3\"),     Info_Format = c(       \"year\", \"year\", \"year\", \"partial_series\", \"partial_series\",       \"partial_series\", \"partial_series\", \"year\", \"year\"     ),     Year = c(1, 2, 3, 1, 2, 3, 4, 2, 4),   )    actual <-     trade_sources |>     expand_trade_sources() |>     dplyr::ungroup()    expect_equal(     dplyr::select(actual, Name, Trade, Info_Format, Year),     expected   ) })"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"r-cmd-check","dir":"Articles","previous_headings":"R package and renv intro","what":"R CMD Check","title":"Follow the workflow","text":"standard tool contains several steps (‘checks’) every project wants package uploaded CRAN repositories must pass. part code workflow, also responsible make check pass, also see Automatic checks Pull Requests section. check known ‘R CMD check’, actually easy run: whole output call rather long, since lists different checks makes, end, issues, output see:  OK, just followed steps guide included example code Writing code Writing tests sections, check ended successfully 0 errors, probably see (among really large output), error like :  problem performing check package, must build . , must know packages dependencies. , just followed everything , never got , built package just include packages. fix , must quick look DESCRIPTION file. file, Imports section make sure dependencies code saved specifically R/ directory. hand, dependencies code written places, tests/ vignettes/ (see one following Writing articles section), included Suggests section DESCRIPTION file. Together two fields tell everyone dependencies package needs work correctly. adding , run devtools::check() confirm fail anymore (least errors). go detail checks performed. slowly learn whenever show real issues code running check tool. Just keep mind one important points tests write also executed , ‘R CMD check’ also fails one tests fail. really want know checks, can read, e.g., detailed list.","code":"devtools::check() Package: whep Title: What the Package Does (One Line, Title Case) Version: 0.0.0.9000 Authors@R:     person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\")) Description: What the package does (one paragraph). License: MIT + file LICENSE Imports:     dplyr,     tidyr Encoding: UTF-8 Roxygen: list(markdown = TRUE) RoxygenNote: 7.3.2 Suggests:     knitr,     rmarkdown,     testthat (>= 3.0.0),     tibble,     ggplot2,     here,     googlesheets4 Config/testthat/edition: 3 VignetteBuilder: knitr URL: https://eduaguilera.github.io/whep/"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"writing-articles","dir":"Articles","previous_headings":"R package and renv intro","what":"Writing articles","title":"Follow the workflow","text":"ever got check popular R package website (e.g. dplyr), may know section called Articles top, open one , see indeed looks like article, mixing natural text code rendering. ideal want make guides package, kind reports general. can probably guess, guide reading built way. Luckily already well integrated R packages workflow, learn make articles easily. Everything want appear Articles section site, included vignettes/ directory package. inside directory, file ends .Rmd extension considered one article. extension name stands ‘R Markdown’, mix R code Markdown text. know Markdown can start , e.g., intro. Following previous example, can create file called trade-sources-coverage.Rmd following code 1 (, thanks Justin): first part code, namely following metadata always present (just change article’s title). see just Markdown, can write R code chunks inside triple backticks, difference code executed, default also able see output rendered article. next chunk (\"r setup\" option) used initialization code may need throughout rest article. time writing really know implications writing \"r setup\" option writing code normal R code chunk (without option), least good practice. Note package loaded package (called whep case). rest code provided just usual Markdown text intertwined usual R code chunks. special case code chunks plots, get see actual plot rendered article, fig.alt option necessary order get R CMD check warning, used text explains rendered image people using screen readers case displayed correctly browser. Now R Markdown article, like visualize . least two useful R commands . first one creates whole documentation website locally computers, automatically opens browser site. simply: now able see article ‘Articles’ section. look something like one can see directly site Trades sources coverage (additional plots). running command , need rerun every time want see changes, since takes bit longer run. can instead use simpler one checks code changes articles reruns changed. completely convinced equivalent, since seems point one failed one worked , something strange (like building guide writing R Markdown inside R Markdown) probably work . pkgdown::build_articles() one still fail error indicating package loaded. likely refers package. Since use code package inside R markdown, package must also installed. running pkgdown::build_site(), think package installed temporary directory execution, maybe work anymore calling pkgdown::build_articles() . case, may want try installing package first via devtools::install(). Note assumes change package code (one R/ directory) actively working articles. , reinstall package every time change something R/ directory. mentioned previous section, also remember include package dependencies article code uses Suggests part DESCRIPTION file, get errors packages found building articles running R CMD check. way can render articles default HTML, browsers use. site perfect can keep date, case need , also able export article PDF. work, first need LaTeX distribution installed. Windows, can try MiKTeX (maybe start ). Remember choose “Yes” asked “missing packages installation fly”, otherwise PDF generation R might fail. LaTeX distribution installed, can build PDF article just single command: output PDF (probably expected) look exactly like one site, style (LaTeX’s style). also valid formats, can look curious, thought PDF output format useful one besides HTML website generation.","code":"--- title: \"Trade sources year coverage\" output: rmarkdown::html_vignette vignette: >   %\\VignetteIndexEntry{Trade sources year coverage}   %\\VignetteEngine{knitr::rmarkdown}   %\\VignetteEncoding{UTF-8} ---  ```_{r, include = FALSE} knitr::opts_chunk$set(   collapse = TRUE,   comment = \"#>\" ) ```  ```_{r setup} library(whep) key_path <- here::here(Sys.getenv(\"GOOGLESHEETS_AUTH_FILE\")) googlesheets4::gs4_auth(path = key_path) ```  First we read the trade sources sheet and build a dataframe where each row accounts for one year. ```_{r} # Step 1: Authentication sheet_url <- \"1UdwgS87x5OsLjNuKaY3JA01GoI5nwsenz62JXCeq0GQ\"  # PART 1: trade_sources FOR TRADE  # Step 2: Rest of Program expanded_trade_sources <-   sheet_url |>   googlesheets4::read_sheet(sheet = \"Final_Sources_Trade\") |>   expand_trade_sources() ```  Now we build some plots.  Plot showing years covered by `expanded_trade_sources`: ```_{r, fig.alt=\"Plot showing years covered by expanded_trade_sources\"} ggplot2::ggplot(   expanded_trade_sources,   ggplot2::aes(y = Trade, x = Year, fill = \"lightblue\") ) +   ggplot2::geom_tile(alpha = .8) +   ggplot2::theme_dark() +   ggplot2::labs(title = \"Source Availability by Country\") +   ggplot2::scale_fill_identity() +   ggplot2::facet_wrap(~Reporter, ncol = 1) ``` --- title: \"Trade sources year coverage\" output: rmarkdown::html_vignette vignette: >   %\\VignetteIndexEntry{Trade sources year coverage}   %\\VignetteEngine{knitr::rmarkdown}   %\\VignetteEncoding{UTF-8} ---  ```_{r, include = FALSE} knitr::opts_chunk$set(   collapse = TRUE,   comment = \"#>\" ) ``` ```_{r setup} library(whep) key_path <- here::here(Sys.getenv(\"GOOGLESHEETS_AUTH_FILE\")) googlesheets4::gs4_auth(path = key_path) ``` pkgdown::build_site() pkgdown::build_articles() rmarkdown::render(\"vignettes/my-vignette.Rmd\", output_format = \"pdf_document\")"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"environment-variables-and-secrets","dir":"Articles","previous_headings":"R package and renv intro","what":"Environment variables and secrets","title":"Follow the workflow","text":"running code, can sometimes customize behavior specifying value variable needs. example, maybe option whose value number 1 5, indicating level debugging (amount output want see console, understand code’s execution). possible way introducing known ‘environment variable’, just value stored name ‘environment’ code runs (R variable, related operating system). loaded code starts running, package workflow allows us quite easily, creating top-level file called .Renviron, can include one variable per line, like : can accessed R code using can also use environment variables constants, whatever feel used several places. file uploaded repository, important contain sensitive information (also known ‘secrets’). also introduced section example code Writing articles section, line: example, Excel sheet must read. using googlesheets4 package, can automatically open browser interactively ask Google account credentials, running check tools, want everything work beginning end without human intervention. achievable providing secret token 2. Since must include secret information .Renviron, workaround instead add just secret file path environment variable read , .Renviron uploaded GitHub secret file, according environment variable found inst/google_cloud_key.json, uploaded. Instead, file uploaded another private storage service, access . examples like one, ideally every developer create use token. specific case token example scope guide, interested probably start gargle package guide. Also, package used easy file path referencing root directory project, paths hard-coded, can read package site.","code":"DEBUG_LEVEL=1 GOOGLESHEETS_AUTH_FILE=inst/google_cloud_key.json Sys.getenv(\"NAME_OF_VARIABLE\") key_path <- here::here(Sys.getenv(\"GOOGLESHEETS_AUTH_FILE\"))"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"r-code-style-and-formatting","dir":"Articles","previous_headings":"R package and renv intro","what":"R code style and formatting","title":"Follow the workflow","text":"conventions good practices write neat code R. followed style guide Tidyverse style guide. can read just skim get grasp conventions. key can checked automatically. tools conform standards let apply necessary changes code just clicking one button. Analogously, also ways check whether code correctly following style , without explicitly changing . context Tidyverse style guide, two points directly match two R packages: styler: applies Tidyverse style guide specific code, chunk, file entire project. example, apply style whole package simply using command: code editors incorporate way . Since likely using RStudio, can finding styler options ‘addins’ drop-:  using renv, seems RStudio shows ‘addins’ drop-options packages included renv, keep mind case want different project, , styler show , means installed current renv environment. important thing keep mind Tidyverse style guide states lines 80 characters, styler package try separate really long single line several lines match 80 character limit. must fix anyway, usual way styler work first manually split code two lines, run styler , can now split rest accordingly. example, : case, use styler may change anything still leave code single line, can help bit sending arguments next line, like : conform standard yet, now trying use styler , able understand going split code accordingly. Depending length line, may leave like (note closing parenthesis goes line): line even longer, split argument line: work now, successfully follow 80 character per line limit. follow limit, fail lintr check (see next point ). lintr: checks whether given code/file/project follows Tidyverse style guide, without making actual changes. responsible making sure check passes (probably using styler explained ), since also automatically checked Pull Requests (see next section guide). check use : default call easy lintr::lint_package(). go detail specific option added , ignore warnings undeclared global variables, false positives using dplyr column names. convenience, following example repository, find call inst/scripts/check_lint.R, want check everything just run script. screenshot can see also options lintr checks Rstudio’s ‘addins’, default perform lintr::lint_package() call without options. Remember added one option check, either find change default behavior just run script included inst/scripts/check_lint.R. , can see screenshot, things can directly RStudio (like running tests building documentation). guide tried provide code editor agnostic approach everything. Since use RStudio , particularly familiar functionalities, think may helpful , can check .","code":"styler::style_pkg() call_my_incredibly_long_function(my_first_long_argument, my_second_long_argument, my_third_long_argument) call_my_incredibly_long_function(   my_first_long_argument, my_second_long_argument, my_third_long_argument) call_my_incredibly_long_function(   my_first_long_argument, my_second_long_argument, my_third_long_argument ) call_my_incredibly_long_function(   my_first_long_argument,   my_second_long_argument,   my_third_long_argument,   my_fourth_long_argument ) lintr::lint_package(   linters = lintr::linters_with_defaults(object_usage_linter = NULL) )"},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"reading-large-files","dir":"Articles","previous_headings":"R package and renv intro","what":"Reading large files","title":"Follow the workflow","text":"Many packages don’t work data per se. Others need data, datasets small enough directly included package (thus code repository). GitHub limit file size 100 MiB, even adding files make working repository noticeably slow. recommendation us consider file large couple megabytes. Assuming package needs work large files, need find workaround storing GitHub. thinking , considered using pins package. encourage read Get started guide, since won’t explain much package , rather implement workflow. pins board like repository files. can even store version history . assume someone already created board . just want use file someone else already added, don’t need following. See documentation whep_read_file(). However, need new file still uploaded, new version existing one, ’ll need steps: Prepare local version data. created script helper us, can found whep repository. fill local data path name data, run script follow instructions. example, set following temporary local folder created whose name data version. must upload folder inside one data’s name public board, assuming write access.  must add one line _pins.yaml file root folder board, like ’s specified instructions. internal file pins package uses keep track files versions.  use version next package release, must also update version inside whep_inputs.csv file: now ’re done. added new file new version, can now access code using whep_read_file() function. ’re wondering manual steps process script doesn’t automate everything, ’s storage service hosted Nextcloud server, easiest way interact outside synced version whole storage locally. thought people might able afford , since don’t upload new data files often, decided manual steps wouldn’t hurt much.","code":"prepare_for_upload( \"~/Downloads/Processing_coefs.csv\", \"processing_coefs\" ) #> Creating new version '20250716T102305Z-f8189' #> ℹ 1. Manually upload the folder /tmp/RtmpHFHNUO/pins-1f3c39c418e0/processi #> ng_coefs/20250716T102305Z-f8189 into your board. Folder path copied to you #> r clipboard. #> ℹ 2. Add the corresponding line #> - processing_coefs/20250716T102305Z-f8189/ #> in _pins.yaml at the end of the 'processing_coefs:' section #> ℹ 3. If you want the package to use this version, add a new row to whep_in #> puts.csv if it's a new file or update the version in the existing row. The #>  version is 20250716T102305Z-f8189. alias,board_url,version ... processing_coefs,https://some/url/to/_pins.yaml,20250716T102305Z-f8189 ..."},{"path":"https://eduaguilera.github.io/whep/articles/workflow-intro.html","id":"automatic-checks-on-pull-requests","dir":"Articles","previous_headings":"R package and renv intro","what":"Automatic checks on Pull Requests","title":"Follow the workflow","text":"point assume already done new code, documented tested , perhaps just created report form article. seen Git intro, now time create Pull Request someone else reviews done. create Pull Request, workflow designed two checks run automatically GitHub, time push changes branch open Pull Request: First, R CMD check performed. seen previous section can run locally, made sure outputs errors. Otherwise, see step failing. , linting check performed. also seen autoformat code, follows R code styling standards, make sure indeed well formatted. Otherwise, see step failing. steps fail, see something like PR page:  Otherwise, checks succeeded, see something like:  Overall, able see bottom PR:  case wondering, checks mentioned actually done single GitHub automatic step (steps called ‘GitHub actions’), say ‘1 successful check’ instead ‘2 successful checks’. However, additional GitHub action , essentially just updates documentation site (PR actually merged main branch). relevant developer makes overall workflow easier, since project documentation automatically updated without human intervention. want know automatic site update can check section R Packages book. Recall PR accepted merged main branch, must also reviewed another developer. good practice make sure GitHub checks pass even asking someone’s reviewing, check failed likely need add code changes fix , code reviewer checked become obsolete, read , better avoid .","code":""},{"path":"https://eduaguilera.github.io/whep/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Catalin Covaci. Author, maintainer. Eduardo Aguilera. Author, copyright holder. João Serra. Contributor. . Funder.","code":""},{"path":"https://eduaguilera.github.io/whep/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Covaci C, Aguilera E (2025). whep: Processing Agro-Environmental Data. R package version 0.1.0.9000, https://eduaguilera.github.io/whep/.","code":"@Manual{,   title = {whep: Processing Agro-Environmental Data},   author = {Catalin Covaci and Eduardo Aguilera},   year = {2025},   note = {R package version 0.1.0.9000},   url = {https://eduaguilera.github.io/whep/}, }"},{"path":[]},{"path":[]},{"path":"https://eduaguilera.github.io/whep/index.html","id":"who-has-eaten-the-planet-the-paths-of-food-systems-beyond-the-safe-and-just-operating-space-1850-2020","dir":"","previous_headings":"Project","what":"Who Has Eaten the Planet? The paths of food systems beyond the safe and just operating space (1850-2020)","title":"Processing Agro-Environmental Data","text":"Food production covers basic human need, simultaneously main driver anthropogenic environmental impacts. impacts resulted transgression, brief period since industrial revolution, planetary boundaries defining safe operating space humanity. rich research literature quantifies last 60 years’ fast, heterogeneous, often unfair development food supply related environmental impacts, depend agro-climatic factors, technology, trade flows, greatly changed different trajectories around world. However, developments lack integrated approach, poorly quantified 1961. WHEP bridge knowledge gaps, assessing “eaten planet” answering questions: environmental impacts food production since 1850? role trade food supply displacing responsibilities impacts? impacts related planetary boundaries, food supply inequality? highly ambitious goals addressed four objectives: Constructing consolidated global country-level annual database agricultural production management, using massive data collation combination modeling. Estimating environmental impacts: greenhouse gas emissions carbon, land, water, nitrogen, phosphorus spatially explicit, integrated, dynamic modeling. Calculating product footprints tracing along international trade chains. Analyzing observed trajectories safe just operating space, assessing drivers, impacts production consumption levels related fair healthy supply. ground-breaking research shed new light environmental history food, opening many new research frontiers, providing necessary information design fair sustainable policies. can also visit European project site.","code":""},{"path":"https://eduaguilera.github.io/whep/index.html","id":"r-package","dir":"","previous_headings":"","what":"R package","title":"Processing Agro-Environmental Data","text":"WHEP project heavily relies data. use R programming language. repository built R package containing functionality think might useful share others part project. also include functions easily downloading data gathered project.","code":""},{"path":"https://eduaguilera.github.io/whep/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Processing Agro-Environmental Data","text":"package constant development. Initial stable releases available CRAN R-universe. can install stable version CRAN: want development version whep, can:","code":"install.packages('whep') # Install from GitHub pak::pak(\"eduaguilera/whep\")  # Install from R-universe install.packages(   \"whep\",   repos = c(\"https://eduaguilera.r-universe.dev\", \"https://cloud.r-project.org\") )"},{"path":"https://eduaguilera.github.io/whep/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Processing Agro-Environmental Data","text":"can read package’s functionalities documentation reference page.","code":""},{"path":"https://eduaguilera.github.io/whep/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Processing Agro-Environmental Data","text":"try follow best coding practices, specifically focused R package creation. process roughly summarized : Use git. Work branch. Track dependencies using renv R package. Add new functionality inside R/ directory functions. Add function documentation. Write clean code. Follow Tidyverse style guide. Write tests code. Create pull requests. Ask review. project starting contributors still learning coding best practices. reason created guide explaining things need previous steps, covering git R package development. can find guide . Anyone welcome contribute, highly recommend go guide become familiar workflow still used .","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Get area codes from area names — add_area_code","title":"Get area codes from area names — add_area_code","text":"Add new column existing tibble corresponding code name. codes assumed defined FABIO model.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get area codes from area names — add_area_code","text":"","code":"add_area_code(table, name_column = \"area_name\", code_column = \"area_code\")"},{"path":"https://eduaguilera.github.io/whep/reference/add_area_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get area codes from area names — add_area_code","text":"table table modified new column. name_column name column table containing names. code_column name output column containing codes.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get area codes from area names — add_area_code","text":"tibble contents table extra column named code_column, contains codes. code match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get area codes from area names — add_area_code","text":"","code":"table <- tibble::tibble(   area_name = c(\"Armenia\", \"Afghanistan\", \"Dummy Country\", \"Albania\") )  add_area_code(table) #> # A tibble: 4 × 2 #>   area_name     area_code #>   <chr>             <dbl> #> 1 Armenia               1 #> 2 Afghanistan           2 #> 3 Dummy Country        NA #> 4 Albania               3  table |>   dplyr::rename(my_area_name = area_name) |>   add_area_code(name_column = \"my_area_name\") #> # A tibble: 4 × 2 #>   my_area_name  area_code #>   <chr>             <dbl> #> 1 Armenia               1 #> 2 Afghanistan           2 #> 3 Dummy Country        NA #> 4 Albania               3  add_area_code(table, code_column = \"my_custom_code\") #> # A tibble: 4 × 2 #>   area_name     my_custom_code #>   <chr>                  <dbl> #> 1 Armenia                    1 #> 2 Afghanistan                2 #> 3 Dummy Country             NA #> 4 Albania                    3"},{"path":"https://eduaguilera.github.io/whep/reference/add_area_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get area names from area codes — add_area_name","title":"Get area names from area codes — add_area_name","text":"Add new column existing tibble corresponding name code. codes assumed defined FABIO model, come FAOSTAT internal codes. Equivalences ISO 3166-1 numeric can found Area Codes CSV zip file can downloaded FAOSTAT. TODO: Think , nice use ISO3 codes enough periods.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get area names from area codes — add_area_name","text":"","code":"add_area_name(table, code_column = \"area_code\", name_column = \"area_name\")"},{"path":"https://eduaguilera.github.io/whep/reference/add_area_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get area names from area codes — add_area_name","text":"table table modified new column. code_column name column table containing codes. name_column name output column containing names.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get area names from area codes — add_area_name","text":"tibble contents table extra column named name_column, contains names. name match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_area_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get area names from area codes — add_area_name","text":"","code":"table <- tibble::tibble(area_code = c(1, 2, 4444, 3))  add_area_name(table) #> # A tibble: 4 × 2 #>   area_code area_name   #>       <dbl> <chr>       #> 1         1 Armenia     #> 2         2 Afghanistan #> 3      4444 NA          #> 4         3 Albania      table |>   dplyr::rename(my_area_code = area_code) |>   add_area_name(code_column = \"my_area_code\") #> # A tibble: 4 × 2 #>   my_area_code area_name   #>          <dbl> <chr>       #> 1            1 Armenia     #> 2            2 Afghanistan #> 3         4444 NA          #> 4            3 Albania      add_area_name(table, name_column = \"my_custom_name\") #> # A tibble: 4 × 2 #>   area_code my_custom_name #>       <dbl> <chr>          #> 1         1 Armenia        #> 2         2 Afghanistan    #> 3      4444 NA             #> 4         3 Albania"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Get commodity balance sheet item codes from item names — add_item_cbs_code","title":"Get commodity balance sheet item codes from item names — add_item_cbs_code","text":"Add new column existing tibble corresponding code commodity balance sheet item name. codes assumed defined FAOSTAT.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get commodity balance sheet item codes from item names — add_item_cbs_code","text":"","code":"add_item_cbs_code(   table,   name_column = \"item_cbs_name\",   code_column = \"item_cbs_code\" )"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get commodity balance sheet item codes from item names — add_item_cbs_code","text":"table table modified new column. name_column name column table containing names. code_column name output column containing codes.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get commodity balance sheet item codes from item names — add_item_cbs_code","text":"tibble contents table extra column named code_column, contains codes. code match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get commodity balance sheet item codes from item names — add_item_cbs_code","text":"","code":"table <- tibble::tibble(   item_cbs_name = c(\"Cottonseed\", \"Eggs\", \"Dummy Item\") ) add_item_cbs_code(table) #> # A tibble: 3 × 2 #>   item_cbs_name item_cbs_code #>   <chr>                 <dbl> #> 1 Cottonseed             2559 #> 2 Eggs                   2744 #> 3 Dummy Item               NA  table |>   dplyr::rename(my_item_cbs_name = item_cbs_name) |>   add_item_cbs_code(name_column = \"my_item_cbs_name\") #> # A tibble: 3 × 2 #>   my_item_cbs_name item_cbs_code #>   <chr>                    <dbl> #> 1 Cottonseed                2559 #> 2 Eggs                      2744 #> 3 Dummy Item                  NA  add_item_cbs_code(table, code_column = \"my_custom_code\") #> # A tibble: 3 × 2 #>   item_cbs_name my_custom_code #>   <chr>                  <dbl> #> 1 Cottonseed              2559 #> 2 Eggs                    2744 #> 3 Dummy Item                NA"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get commodity balance sheet item names from item codes — add_item_cbs_name","title":"Get commodity balance sheet item names from item codes — add_item_cbs_name","text":"Add new column existing tibble corresponding name commodity balance sheet item code. codes assumed defined FAOSTAT.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get commodity balance sheet item names from item codes — add_item_cbs_name","text":"","code":"add_item_cbs_name(   table,   code_column = \"item_cbs_code\",   name_column = \"item_cbs_name\" )"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get commodity balance sheet item names from item codes — add_item_cbs_name","text":"table table modified new column. code_column name column table containing codes. name_column name output column containing names.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get commodity balance sheet item names from item codes — add_item_cbs_name","text":"tibble contents table extra column named name_column, contains names. name match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_cbs_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get commodity balance sheet item names from item codes — add_item_cbs_name","text":"","code":"table <- tibble::tibble(item_cbs_code = c(2559, 2744, 9876)) add_item_cbs_name(table) #> # A tibble: 3 × 2 #>   item_cbs_code item_cbs_name #>           <dbl> <chr>         #> 1          2559 Cottonseed    #> 2          2744 Eggs          #> 3          9876 NA             table |>   dplyr::rename(my_item_cbs_code = item_cbs_code) |>   add_item_cbs_name(code_column = \"my_item_cbs_code\") #> # A tibble: 3 × 2 #>   my_item_cbs_code item_cbs_name #>              <dbl> <chr>         #> 1             2559 Cottonseed    #> 2             2744 Eggs          #> 3             9876 NA             add_item_cbs_name(table, name_column = \"my_custom_name\") #> # A tibble: 3 × 2 #>   item_cbs_code my_custom_name #>           <dbl> <chr>          #> 1          2559 Cottonseed     #> 2          2744 Eggs           #> 3          9876 NA"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Get production item codes from item names — add_item_prod_code","title":"Get production item codes from item names — add_item_prod_code","text":"Add new column existing tibble corresponding code production item name. codes assumed defined FAOSTAT.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get production item codes from item names — add_item_prod_code","text":"","code":"add_item_prod_code(   table,   name_column = \"item_prod_name\",   code_column = \"item_prod_code\" )"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get production item codes from item names — add_item_prod_code","text":"table table modified new column. name_column name column table containing names. code_column name output column containing codes.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get production item codes from item names — add_item_prod_code","text":"tibble contents table extra column named code_column, contains codes. code match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_code.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get production item codes from item names — add_item_prod_code","text":"","code":"table <- tibble::tibble(   item_prod_name = c(\"Rice\", \"Cabbages\", \"Dummy Item\") ) add_item_prod_code(table) #> # A tibble: 3 × 2 #>   item_prod_name item_prod_code #>   <chr>                   <dbl> #> 1 Rice                       27 #> 2 Cabbages                  358 #> 3 Dummy Item                 NA  table |>   dplyr::rename(my_item_prod_name = item_prod_name) |>   add_item_prod_code(name_column = \"my_item_prod_name\") #> # A tibble: 3 × 2 #>   my_item_prod_name item_prod_code #>   <chr>                      <dbl> #> 1 Rice                          27 #> 2 Cabbages                     358 #> 3 Dummy Item                    NA  add_item_prod_code(table, code_column = \"my_custom_code\") #> # A tibble: 3 × 2 #>   item_prod_name my_custom_code #>   <chr>                   <dbl> #> 1 Rice                       27 #> 2 Cabbages                  358 #> 3 Dummy Item                 NA"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get production item names from item codes — add_item_prod_name","title":"Get production item names from item codes — add_item_prod_name","text":"Add new column existing tibble corresponding name production item code. codes assumed defined FAOSTAT.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get production item names from item codes — add_item_prod_name","text":"","code":"add_item_prod_name(   table,   code_column = \"item_prod_code\",   name_column = \"item_prod_name\" )"},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get production item names from item codes — add_item_prod_name","text":"table table modified new column. code_column name column table containing codes. name_column name output column containing names.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get production item names from item codes — add_item_prod_name","text":"tibble contents table extra column named name_column, contains names. name match, NA included.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/add_item_prod_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get production item names from item codes — add_item_prod_name","text":"","code":"table <- tibble::tibble(item_prod_code = c(27, 358, 12345)) add_item_prod_name(table) #> # A tibble: 3 × 2 #>   item_prod_code item_prod_name #>            <dbl> <chr>          #> 1             27 Rice           #> 2            358 Cabbages       #> 3          12345 NA              table |>   dplyr::rename(my_item_prod_code = item_prod_code) |>   add_item_prod_name(code_column = \"my_item_prod_code\") #> # A tibble: 3 × 2 #>   my_item_prod_code item_prod_name #>               <dbl> <chr>          #> 1                27 Rice           #> 2               358 Cabbages       #> 3             12345 NA              add_item_prod_name(table, name_column = \"my_custom_name\") #> # A tibble: 3 × 2 #>   item_prod_code my_custom_name #>            <dbl> <chr>          #> 1             27 Rice           #> 2            358 Cabbages       #> 3          12345 NA"},{"path":"https://eduaguilera.github.io/whep/reference/build_supply_use.html","id":null,"dir":"Reference","previous_headings":"","what":"Supply and use tables — build_supply_use","title":"Supply and use tables — build_supply_use","text":"Create table processes, inputs (use) outputs (supply).","code":""},{"path":"https://eduaguilera.github.io/whep/reference/build_supply_use.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supply and use tables — build_supply_use","text":"","code":"build_supply_use(   cbs_version = NULL,   feed_intake_version = NULL,   primary_prod_version = NULL,   primary_residues_version = NULL,   processing_coefs_version = NULL )"},{"path":"https://eduaguilera.github.io/whep/reference/build_supply_use.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supply and use tables — build_supply_use","text":"cbs_version File version passed get_wide_cbs() call. feed_intake_version File version passed get_feed_intake() call. primary_prod_version File version passed get_primary_production() call. primary_residues_version File version passed get_primary_residues() call. processing_coefs_version File version passed get_processing_coefs() call.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/build_supply_use.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supply and use tables — build_supply_use","text":"tibble supply use data processes. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). proc_group: type process taking place. can one : crop_production: Production crops residues, e.g. rice production, coconut production, etc. husbandry: Animal husbandry, e.g. dairy cattle husbandry, non-dairy cattle husbandry, layers chickens farming, etc. processing: Derived subproducts obtained processing items. items used inputs non-zero processing use commodity balance sheet. See get_wide_cbs() details. process single input. processes like olive oil extraction soyabean oil extraction might make sense. Others like alcohol production need multiple inputs (e.g. multiple crops work), data process like alcohol production rather virtual process like 'Wheat products processing', giving possible outputs. constraint data obtained might improved future. See get_processing_coefs() details. proc_cbs_code: code main item process taking place. Together proc_group, two columns uniquely represent process. main item predictable depending value proc_group: crop_production: code item seed usage () reported commodity balance sheet (see get_wide_cbs() ). example, rice code rice production process cottonseed code cotton production one. husbandry: code farmed animal, e.g. bees beekeeping, non-dairy cattle non-dairy cattle husbandry, etc. processing: code item used input, .e., one processed get derived products. uniquely defines process within group nature data used, can see get_processing_coefs(). code details see e.g. add_item_cbs_name(). item_cbs_code: code item produced used process. Note might value proc_cbs_code, e.g., rice production process row defining amount rice produced amount rice seed input, might also different value, e.g. row defining amount straw residue rice production. code details see e.g. add_item_cbs_name(). type: Can two values: use: given item input process. supply: given item output process. value: Quantity tonnes.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/build_supply_use.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supply and use tables — build_supply_use","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default versions (i.e. no arguments). build_supply_use(   cbs_version = \"20250721T132006Z-8ea47\",   feed_intake_version = \"20250721T143825Z-c1313\",   primary_prod_version = \"20250721T145805Z-8e12a\",   primary_residues_version = \"20250721T150132Z-dfd94\",   processing_coefs_version = \"20250721T143403Z-216d7\" ) #> ℹ Fetching files for primary_prod... #> ℹ Fetching files for crop_residues... #> ℹ Fetching files for commodity_balance_sheet... #> ℹ Fetching files for feed_intake... #> ℹ Fetching files for processing_coefs... #> # A tibble: 27,914 × 7 #>     year area_code proc_group      proc_cbs_code item_cbs_code type       value #>    <dbl>     <dbl> <chr>                   <dbl>         <dbl> <chr>      <dbl> #>  1  1965        38 crop_production          2645          2645 supply     4460  #>  2  1984        50 crop_production          2625          2625 supply       45  #>  3  2003       131 crop_production          2577          2577 supply 13354800  #>  4  1996       235 crop_production          2517          2517 supply     1600  #>  5  2021        10 crop_production          2511          2511 supply 31922555. #>  6  1970        41 crop_production          2605          2605 supply     7000  #>  7  1997       222 crop_production          2605          2605 supply    12000  #>  8  2003        59 crop_production          2662          2662 supply     2049. #>  9  1966       236 crop_production          2549          2549 supply     5891  #> 10  1976       105 crop_production          2533          2533 supply      800  #> # ℹ 27,904 more rows"},{"path":"https://eduaguilera.github.io/whep/reference/expand_trade_sources.html","id":null,"dir":"Reference","previous_headings":"","what":"Trade data sources — expand_trade_sources","title":"Trade data sources — expand_trade_sources","text":"Create new dataframe row year range one row single year, effectively 'expanding' whole year range.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/expand_trade_sources.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trade data sources — expand_trade_sources","text":"","code":"expand_trade_sources(trade_sources)"},{"path":"https://eduaguilera.github.io/whep/reference/expand_trade_sources.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trade data sources — expand_trade_sources","text":"trade_sources tibble dataframe row contains year range.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/expand_trade_sources.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trade data sources — expand_trade_sources","text":"tibble dataframe row corresponds single year given source.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/expand_trade_sources.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trade data sources — expand_trade_sources","text":"","code":"trade_sources <- tibble::tibble(   Name = c(\"a\", \"b\", \"c\"),   Trade = c(\"t1\", \"t2\", \"t3\"),   Info_Format = c(\"year\", \"partial_series\", \"year\"),   Timeline_Start = c(1, 1, 2),   Timeline_End = c(3, 4, 5),   Timeline_Freq = c(1, 1, 2),   `Imp/Exp` = \"Imp\",   SACO_link = NA, ) expand_trade_sources(trade_sources) #> # A tibble: 9 × 12 #> # Groups:   No [3] #>      No  Year Name  Trade Info_Format  Timeline_Start Timeline_End Timeline_Freq #>   <int> <dbl> <chr> <chr> <chr>                 <dbl>        <dbl>         <dbl> #> 1     1     1 a_1   t1    year                      1            3             1 #> 2     1     2 a_2   t1    year                      1            3             1 #> 3     1     3 a_3   t1    year                      1            3             1 #> 4     2     1 b     t2    partial_ser…              1            4             1 #> 5     2     2 b     t2    partial_ser…              1            4             1 #> 6     2     3 b     t2    partial_ser…              1            4             1 #> 7     2     4 b     t2    partial_ser…              1            4             1 #> 8     3     2 c_2   t3    year                      2            5             2 #> 9     3     4 c_4   t3    year                      2            5             2 #> # ℹ 4 more variables: `Imp/Exp` <chr>, SACO_link <lgl>, ImpExp <chr>, #> #   In_Saco <int>"},{"path":"https://eduaguilera.github.io/whep/reference/get_bilateral_trade.html","id":null,"dir":"Reference","previous_headings":"","what":"Bilateral trade data — get_bilateral_trade","title":"Bilateral trade data — get_bilateral_trade","text":"Reports trade pairs countries given years.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_bilateral_trade.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bilateral trade data — get_bilateral_trade","text":"","code":"get_bilateral_trade(trade_version = NULL, cbs_version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_bilateral_trade.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bilateral trade data — get_bilateral_trade","text":"trade_version File version used bilateral trade input. See whep_inputs version details. cbs_version File version passed get_wide_cbs() call.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_bilateral_trade.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bilateral trade data — get_bilateral_trade","text":"tibble reported trade countries. efficient memory usage, tibble exactly tidy format. contains following columns: year: year recorded event occurred. item_cbs_code: FAOSTAT internal code item traded. code details see e.g. add_item_cbs_name(). bilateral_trade: Square matrix NxN dimensions N total number countries considered. matrix row column names exactly equal represent country codes. Row name: code country data . code details see e.g. add_area_name(). Column name: FAOSTAT internal code country importing item. See row name explanation . m matrix, value m[\"\", \"B\"] trade tonnes country \"\" country \"B\", corresponding year item. matrix can considered balanced. means: sum values row \"\", \"\" country, match total exports country \"\" reported commodity balance sheet (considered accurate totals). sum values column \"\", \"\" country, match total imports country \"\" reported commodity balance sheet (considered accurate totals). sums may exactly expected values precision issues /iterative proportional fitting algorithm converging fast enough, relatively close desired totals. step step approach obtain data tries follow FABIO model explained . steps performed separately group year item. FAOSTAT reported bilateral trade, sometimes two values one trade flow: exported amount claimed reporter country import amount claimed partner country. , export data preferred, .e., country \"\" says exported X tonnes country \"B\" country \"B\" claims got Y tonnes country \"\", trust export data X. choice needed exists reported amount sides. Otherwise, single existing report chosen. Complete country data, , add missing combinations country trade NAs, estimated later. matrix form, increase memory usage since build matrix anyway (balancing algorithm), empty parts also take memory. also done total imports/exports commodity balance sheet, directly filled 0s instead. total imports exports commodity balance sheet balanced downscaling largest two match lowest. done following way: total_imports > total_exports: Set import total_exports * import / total_import. total_exports > total_exports: Set export total_exports * export / total_export. missing data matrix must estimated. done like : pair exporter importer j, estimate bilateral trade m[, j] using export shares import shares j commodity balance sheet: est_1 <- exports[] * imports[j] / sum(imports), .e., total exports country spread among countries' import shares. est_2 <- imports[j] * exports[] / sum(exports), .e. total imports country j spread among countries' export shares. est <- (est_1 + est_2) / 2, .e., mean estimates. computations, exports imports original values balanced. estimates data already existed (.e. non-NA) discarded. ones left, row (.e. exporter country), get difference balanced total export sum original non-estimated data. result gap can actually fill estimates, get past reported total export. sum non-discarded estimates larger, must downscaled spread computing gap * non_discarded_estimate / sum(non_discarded_estimates). estimates divided trust factor, sense rely whole value, thinking non-present value might actually specific trade 0, overestimate much. chosen factor 10%, 10% estimate's value actually used fill NA original bilateral trade matrix. matrix balanced, mentioned , using iterative proportional fitting algorithm. target sums rows columns respectively balanced exports imports computed commodity balance sheet. algorithm performed directly using mipfp R package.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_bilateral_trade.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bilateral trade data — get_bilateral_trade","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default versions (i.e. no arguments). get_bilateral_trade(   trade_version = \"20250721T141553Z-5707e\",   cbs_version = \"20250721T132006Z-8ea47\" ) #> ℹ Fetching files for commodity_balance_sheet... #> ℹ Fetching files for bilateral_trade... #> # A tibble: 72 × 3 #>     year item_cbs_code bilateral_trade   #>    <int>         <dbl> <list>            #>  1  1995          2630 <dbl [187 × 187]> #>  2  2001          2733 <dbl [187 × 187]> #>  3  2005          2671 <dbl [187 × 187]> #>  4  2017          2513 <dbl [187 × 187]> #>  5  2011          2601 <dbl [187 × 187]> #>  6  2011          2655 <dbl [187 × 187]> #>  7  2015          2558 <dbl [187 × 187]> #>  8  2014          2577 <dbl [187 × 187]> #>  9  2000          2625 <dbl [187 × 187]> #> 10  2018          2661 <dbl [187 × 187]> #> # ℹ 62 more rows"},{"path":"https://eduaguilera.github.io/whep/reference/get_faostat_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","title":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","text":"Important: Dynamically allows introduction subsets \"...\". Note: overhead individually scraping FAOSTAT code QCL crop data; fine.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_faostat_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","text":"","code":"get_faostat_data(activity_data, ...)"},{"path":"https://eduaguilera.github.io/whep/reference/get_faostat_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","text":"activity_data activity data required FAOSTAT; needs one c('livestock','crop_area','crop_yield','crop_production'). ... can whichever column name get_faostat_bulk, particularly year, area ISO3_CODE.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_faostat_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","text":"data.frame FAOSTAT activity_data; default years countries.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_faostat_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrapes activity_data from FAOSTAT and slightly post-processes it — get_faostat_data","text":"","code":"# \\donttest{ get_faostat_data(\"livestock\", year = 2010, area = \"Portugal\") #> Loading required package: FAOSTAT #> Warning: Duplicated ISO3_CODE matched, double check the data #> Warning: Certain ISO3_CODE were not matched. #> Warning: Please check the correct China has been specified. #> # A tibble: 18 × 7 #>    area     item               element  year     value unit  ISO3_CODE #>    <chr>    <chr>              <chr>   <int>     <dbl> <chr> <chr>     #>  1 Portugal Asses              stocks   2010    13538. An    PRT       #>  2 Portugal Cattle, dairy      stocks   2010   243000  An    PRT       #>  3 Portugal Cattle, non-dairy  stocks   2010  1204000  An    PRT       #>  4 Portugal Chickens, broilers stocks   2010 25733000  An    PRT       #>  5 Portugal Chickens, layers   stocks   2010  6500000  An    PRT       #>  6 Portugal Goats              stocks   2010   419000  An    PRT       #>  7 Portugal Horses             stocks   2010    97000  An    PRT       #>  8 Portugal Mules and hinnies  stocks   2010     6273. An    PRT       #>  9 Portugal Sheep              stocks   2010  2226000  An    PRT       #> 10 Portugal Swine, breeding    stocks   2010   191700  An    PRT       #> 11 Portugal Swine, market      stocks   2010  1725300  An    PRT       #> 12 Portugal Turkeys            stocks   2010  2231878. An    PRT       #> 13 Portugal Cattle             stocks   2010  1447000  An    PRT       #> 14 Portugal Chickens           stocks   2010 32233000  An    PRT       #> 15 Portugal Mules and Asses    stocks   2010    19812. An    PRT       #> 16 Portugal Poultry Birds      stocks   2010 34464878. An    PRT       #> 17 Portugal Sheep and Goats    stocks   2010  2645000  An    PRT       #> 18 Portugal Swine              stocks   2010  1917000  An    PRT       # }"},{"path":"https://eduaguilera.github.io/whep/reference/get_feed_intake.html","id":null,"dir":"Reference","previous_headings":"","what":"Livestock feed intake — get_feed_intake","title":"Livestock feed intake — get_feed_intake","text":"Get amount items used feeding livestock.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_feed_intake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Livestock feed intake — get_feed_intake","text":"","code":"get_feed_intake(version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_feed_intake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Livestock feed intake — get_feed_intake","text":"version File version use input. See whep_inputs details.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_feed_intake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Livestock feed intake — get_feed_intake","text":"tibble feed intake data. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). live_anim_code: Commodity balance sheet code type livestock fed. code details see e.g. add_item_cbs_name(). item_cbs_code: code item used feeding animal. code details see e.g. add_item_cbs_name(). feed_type: type item fed. can one : animals: Livestock product, e.g. Bovine Meat, Butter, Ghee, etc. crops: Crop product, e.g. Vegetables, , Oats, etc. residues: Crop residue, e.g. Straw, Fodder legumes, etc. grass: Grass, e.g. Grassland, Temporary grassland, etc. scavenging: residues. Single Scavenging item. supply: computed amount tonnes item fed animal, sharing total item feed use Commodity Balance Sheet among livestock. intake: actual amount tonnes animal needs, can less theoretical used amount supply. intake_dry_matter: amount specified intake considering dry matter, less intake. loss: amount used feed. supply - intake. loss_share: percent lost. loss / supply.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_feed_intake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Livestock feed intake — get_feed_intake","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default version (i.e. no arguments). get_feed_intake(version = \"20250721T143825Z-c1313\") #> ℹ Fetching files for feed_intake... #> # A tibble: 10,000 × 10 #>     year area_code live_anim_code item_cbs_code feed_type   supply   intake #>    <dbl>     <dbl>          <dbl>         <dbl> <chr>        <dbl>    <dbl> #>  1  1983        15           1096          2102 crops         21.1     18.9 #>  2  1985       251           1016          3000 grass     269616.  269616.  #>  3  2021       222           1052          2781 animals       35.9     32.3 #>  4  2017       105           1079          2598 crops       1727.    1554.  #>  5  2000        39           1053          2106 residues   12662.   11396.  #>  6  1968        84           1016          2002 residues    3015.    2714.  #>  7  2008       170           1053          2595 crops       4294.    3864.  #>  8  2015         3            976          2101 crops         40.1     36.1 #>  9  2002        79           1052          2558 crops       3788.    3409.  #> 10  2014        41           1068          2518 crops     148505.  133654.  #> # ℹ 9,990 more rows #> # ℹ 3 more variables: intake_dry_matter <dbl>, loss <dbl>, loss_share <dbl>"},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_production.html","id":null,"dir":"Reference","previous_headings":"","what":"Primary items production — get_primary_production","title":"Primary items production — get_primary_production","text":"Get amount crops, livestock livestock products.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_production.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Primary items production — get_primary_production","text":"","code":"get_primary_production(version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_production.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Primary items production — get_primary_production","text":"version File version use input. See whep_inputs details.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_production.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Primary items production — get_primary_production","text":"tibble item production data. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). item_prod_code: FAOSTAT internal code produced item. item_cbs_code: FAOSTAT internal code commodity balance sheet item. commodity balance sheet contains aggregated version production items. field code corresponding aggregated item. live_anim_code: Commodity balance sheet code type livestock produces livestock product. can : NA: entry livestock product. Non-NA: code livestock type. name can also retrieved using add_item_cbs_name(). unit: Measurement unit data. , keep mind three groups items: crops (e.g. Apples products, Beans...), livestock (e.g. Cattle, dairy, Goats...) livestock products (e.g. Poultry Meat, Offals, Edible...). unit can one : tonnes: Available crops livestock products. ha: Hectares, available crops. t_ha: Tonnes per hectare, available crops. heads: Number animals, available livestock. LU: Standard Livestock Unit measure, available livestock. t_head: tonnes per head, available livestock products. t_LU: tonnes per Livestock Unit, available livestock products. value: amount item produced, measured unit.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_production.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Primary items production — get_primary_production","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default version (i.e. no arguments). get_primary_production(version = \"20250721T145805Z-8e12a\") #> ℹ Fetching files for primary_prod... #> # A tibble: 10,000 × 7 #>     year area_code item_prod_code item_cbs_code live_anim_code unit       value #>    <dbl>     <dbl>          <dbl>         <dbl>          <dbl> <chr>      <dbl> #>  1  1965        38            720          2645             NA tonnes 4460      #>  2  2019       213           1062          2744           1052 t_LU    581.     #>  3  1999        39            826          2671             NA t_ha      1.45   #>  4  1968       114            772           772             NA t_ha      1      #>  5  2001       236            406          2605             NA ha     1591      #>  6  1984        50            603          2625             NA tonnes   45      #>  7  1981       136            567          2605             NA ha      720      #>  8  2005        40            547          2625             NA ha     4532      #>  9  2009       215            977          2732            976 t_LU      0.0321 #> 10  1996       158            191          2549             NA ha      270      #> # ℹ 9,990 more rows"},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_residues.html","id":null,"dir":"Reference","previous_headings":"","what":"Crop residue items — get_primary_residues","title":"Crop residue items — get_primary_residues","text":"Get type amount residue produced crop production item.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_residues.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crop residue items — get_primary_residues","text":"","code":"get_primary_residues(version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_residues.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crop residue items — get_primary_residues","text":"version File version use input. See whep_inputs details.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_residues.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crop residue items — get_primary_residues","text":"tibble crop residue data. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). item_cbs_code_crop: FAOSTAT internal code commodity balance sheet item. crop generating residue. item_cbs_code_residue: FAOSTAT internal code commodity balance sheet item. obtained residue. commodity balance sheet, can three different items right now: 2105: Straw 2106: crop residues 2107: Firewood actually FAOSTAT defined items, custom defined us. necessary, FAOSTAT codes extended needs. value: amount residue produced, measured tonnes.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_primary_residues.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crop residue items — get_primary_residues","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default version (i.e. no arguments). get_primary_residues(version = \"20250721T150132Z-dfd94\") #> ℹ Fetching files for crop_residues... #> # A tibble: 4,504 × 5 #>     year area_code item_cbs_code_crop item_cbs_code_residue    value #>    <dbl>     <dbl>              <dbl>                 <dbl>    <dbl> #>  1  2003        53               2570                  2107  19903.  #>  2  2008        NA               2551                  2107 222849.  #>  3  2001        NA               2605                  2106   2309.  #>  4  1981       136               2605                  2106    421.  #>  5  2019       121               2605                  2106  19703.  #>  6  2005        40               2625                  2107   5904.  #>  7  1978       166               2605                  2106    828.  #>  8  1996       158               2549                  2105     92.9 #>  9  1976        10               2612                  2107   5401.  #> 10  2012         3               2605                  2106  12086.  #> # ℹ 4,494 more rows"},{"path":"https://eduaguilera.github.io/whep/reference/get_processing_coefs.html","id":null,"dir":"Reference","previous_headings":"","what":"Processed products share factors — get_processing_coefs","title":"Processed products share factors — get_processing_coefs","text":"Reports quantities commodity balance sheet items used processing quantities corresponding processed output items.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_processing_coefs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Processed products share factors — get_processing_coefs","text":"","code":"get_processing_coefs(version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_processing_coefs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Processed products share factors — get_processing_coefs","text":"version File version use input. See whep_inputs details.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_processing_coefs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Processed products share factors — get_processing_coefs","text":"tibble quantities processed product. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). item_cbs_code_to_process: FAOSTAT internal code one items processed give subproduct items. code details see e.g. add_item_cbs_name(). value_to_process: tonnes item processed. matches amount found processing column data obtained get_wide_cbs(). item_cbs_code_processed: FAOSTAT internal code one subproduct items obtained processing. code details see e.g. add_item_cbs_name(). initial_conversion_factor: estimate number tonnes item_cbs_code_processed obtained tonne item_cbs_code_to_process. used compute final_conversion_factor, leaves everything balanced. TODO: explain computed. initial_value_processed: first estimate number tonnes item_cbs_code_processed obtained item_cbs_code_to_process. computed value_to_process * initial_conversion_factor. conversion_factor_scaling: computed scaling needed adapt initial_conversion_factor get final balanced total subproduct quantities. TODO: explain computed. final_conversion_factor: final used estimate number tonnes item_cbs_code_processed obtained tonne item_cbs_code_to_process. computed initial_conversion_factor * conversion_factor_scaling. final_value_processed: final estimate number tonnes item_cbs_code_processed obtained item_cbs_code_to_process. computed initial_value_processed * final_conversion_factor. final data obtained, quantities final_value_processed balanced following sense: total sum final_value_processed unique tuple (year, area_code, item_cbs_code_processed) exactly quantity reported year, country item_cbs_code_processed item production column obtained get_wide_cbs(). primary products, amount 'production' actually amount subproduct obtained. TODO: Fix data hold.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_processing_coefs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Processed products share factors — get_processing_coefs","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default version (i.e. no arguments). get_processing_coefs(version = \"20250721T143403Z-216d7\") #> ℹ Fetching files for processing_coefs... #> # A tibble: 5,000 × 10 #>     year area_code item_cbs_code_to_process value_to_process #>    <dbl>     <dbl>                    <dbl>            <dbl> #>  1  2012        38                     2561           1097.  #>  2  2009        28                     2537         372812.  #>  3  1992       110                     2537        3581000   #>  4  1965        51                     2555          25085   #>  5  2008        52                     2537          94467.  #>  6  2000       166                     2544          25108   #>  7  1962       156                     2513          39031.  #>  8  2010        28                     2537         374089.  #>  9  1966       121                     2557             23.8 #> 10  2015       231                     2559        1142825.  #> # ℹ 4,990 more rows #> # ℹ 6 more variables: item_cbs_code_processed <dbl>, #> #   initial_conversion_factor <dbl>, initial_value_processed <dbl>, #> #   conversion_factor_scaling <dbl>, final_conversion_factor <dbl>, #> #   final_value_processed <dbl>"},{"path":"https://eduaguilera.github.io/whep/reference/get_wide_cbs.html","id":null,"dir":"Reference","previous_headings":"","what":"Commodity balance sheet data — get_wide_cbs","title":"Commodity balance sheet data — get_wide_cbs","text":"States supply use parts commodity balance sheet (CBS) item.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_wide_cbs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Commodity balance sheet data — get_wide_cbs","text":"","code":"get_wide_cbs(version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/get_wide_cbs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Commodity balance sheet data — get_wide_cbs","text":"version File version use input. See whep_inputs details.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_wide_cbs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Commodity balance sheet data — get_wide_cbs","text":"tibble commodity balance sheet data wide format. contains following columns: year: year recorded event occurred. area_code: code country data . code details see e.g. add_area_name(). item_cbs_code: FAOSTAT internal code item. code details see e.g. add_item_cbs_name(). columns quantities (measured tonnes), total supply total use balanced. supply: production: Produced locally. import: Obtained importing countries. stock_retrieval: Available net stock previous years. ease, one stock column included supply. value positive, stock quantity available supply. Otherwise, means larger quantity stored later years used supply, deduce total supply. Since case negative, total supply still computed sum . use: food: Food humans. feed: Food animals. export: Released export countries. seed: Intended new production. processing: product used obtain subproducts. other_uses: use included ones. additional column domestic_supply computed total use excluding export.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/get_wide_cbs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Commodity balance sheet data — get_wide_cbs","text":"","code":"# Note: These are smaller samples to show outputs, not the real data. # For all data, call the function with default version (i.e. no arguments). get_wide_cbs(version = \"20250721T132006Z-8ea47\") #> ℹ Fetching files for commodity_balance_sheet... #> # A tibble: 9,959 × 13 #>     year area_code item_cbs_code processing production other_uses  feed  seed #>    <int>     <int>         <dbl>      <dbl>      <dbl>      <dbl> <dbl> <dbl> #>  1  1968       216          2775          0         0           0     0     0 #>  2  2011        57          2731          0    297800           0     0     0 #>  3  1962        51          2590          0         0           0     0     0 #>  4  1973        48          2737          0      6656.          0     0     0 #>  5  1987       171          2106          0         0           0     0     0 #>  6  2012       129          2105          0         0           0     0     0 #>  7  1980       162          2737          0         0           0     0     0 #>  8  2001        97          2781          0         0           0     0     0 #>  9  2021       250          2613          0         0           0     0     0 #> 10  1968       202          2807          0         0           0     0     0 #> # ℹ 9,949 more rows #> # ℹ 5 more variables: import <dbl>, domestic_supply <dbl>, food <dbl>, #> #   export <dbl>, stock_retrieval <dbl>"},{"path":"https://eduaguilera.github.io/whep/reference/items_cbs.html","id":null,"dir":"Reference","previous_headings":"","what":"Commodity balance sheet items — items_cbs","title":"Commodity balance sheet items — items_cbs","text":"Defines name/code correspondences commodity balance sheet (CBS) items.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/items_cbs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Commodity balance sheet items — items_cbs","text":"","code":"items_cbs"},{"path":"https://eduaguilera.github.io/whep/reference/items_cbs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Commodity balance sheet items — items_cbs","text":"tibble row corresponds one CBS item. contains following columns: item_cbs_code: numeric code used refer CBS item. item_cbs_name: natural language name item. item_type: ad-hoc grouping items. work progress evolving depending needs, now two possible values: livestock: CBS item represents live animal. : previous groups.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/items_cbs.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Commodity balance sheet items — items_cbs","text":"Inspired FAOSTAT data.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/items_prod.html","id":null,"dir":"Reference","previous_headings":"","what":"Primary production items — items_prod","title":"Primary production items — items_prod","text":"Defines name/code correspondences production items.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/items_prod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Primary production items — items_prod","text":"","code":"items_prod"},{"path":"https://eduaguilera.github.io/whep/reference/items_prod.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Primary production items — items_prod","text":"tibble row corresponds one production item. contains following columns: item_prod_code: numeric code used refer item. item_prod_name: natural language name item. item_type: ad-hoc grouping items. work progress evolving depending needs, now two possible values: crop_product: CBS item represents crop product. : previous groups.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/items_prod.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Primary production items — items_prod","text":"Inspired FAOSTAT data.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/polities.html","id":null,"dir":"Reference","previous_headings":"","what":"Polities — polities","title":"Polities — polities","text":"Defines name/code correspondences polities (political entities).","code":""},{"path":"https://eduaguilera.github.io/whep/reference/polities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Polities — polities","text":"","code":"polities"},{"path":"https://eduaguilera.github.io/whep/reference/polities.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Polities — polities","text":"tibble row corresponds one polity. contains following columns: TODO: polities Pull Request, coming soon","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep-package.html","id":null,"dir":"Reference","previous_headings":"","what":"whep: Processing Agro-Environmental Data — whep-package","title":"whep: Processing Agro-Environmental Data — whep-package","text":"set tools processing analyzing data developed context \"Eaten Planet\" (WHEP) project, funded European Research Council (ERC). details multi-regional input–output model \"Food Agriculture Biomass Input–Output\" (FABIO) see Bruckner et al. (2019) doi:10.1021/acs.est.9b03554 .","code":""},{"path":[]},{"path":"https://eduaguilera.github.io/whep/reference/whep-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"whep: Processing Agro-Environmental Data — whep-package","text":"Maintainer: Catalin Covaci catalin.covaci@csic.es (ORCID) Authors: Eduardo Aguilera eduardo.aguilera@csic.es (ORCID) [copyright holder] contributors: João Serra jserra@agro.au.dk (ORCID) [contributor] European Research Council [funder]","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_inputs.html","id":null,"dir":"Reference","previous_headings":"","what":"External inputs — whep_inputs","title":"External inputs — whep_inputs","text":"information needed accessing external datasets used inputs modeling.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_inputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"External inputs — whep_inputs","text":"","code":"whep_inputs"},{"path":"https://eduaguilera.github.io/whep/reference/whep_inputs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"External inputs — whep_inputs","text":"tibble row corresponds one external input dataset. contains following columns: alias: internal name used refer dataset, expected name trying get dataset whep_read_file(). board_url: public static URL data found, following concept board pins package, use storing input datasets. version: specific version dataset, defined pins package. version string similar \"20250714T123343Z-114b5\". version one used default version specified calling whep_read_file(). want use different one, can find available versions file using whep_list_file_versions().","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_inputs.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"External inputs — whep_inputs","text":"Created package authors.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_list_file_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"Input file versions — whep_list_file_versions","title":"Input file versions — whep_list_file_versions","text":"Lists existing versions input file whep_inputs.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_list_file_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Input file versions — whep_list_file_versions","text":"","code":"whep_list_file_versions(file_alias)"},{"path":"https://eduaguilera.github.io/whep/reference/whep_list_file_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Input file versions — whep_list_file_versions","text":"file_alias Internal name requested file. can find possible values whep_inputs dataset.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_list_file_versions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Input file versions — whep_list_file_versions","text":"tibble row version. details format, see pins::pin_versions().","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_list_file_versions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Input file versions — whep_list_file_versions","text":"","code":"whep_list_file_versions(\"read_example\") #> # A tibble: 3 × 3 #>   version                created             hash  #>   <chr>                  <dttm>              <chr> #> 1 20250721T152646Z-ce61b 2025-07-21 15:26:46 ce61b #> 2 20250721T152756Z-f00da 2025-07-21 15:27:56 f00da #> 3 20250721T152854Z-7cc8f 2025-07-21 15:28:54 7cc8f"},{"path":"https://eduaguilera.github.io/whep/reference/whep_read_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Download, cache and read files — whep_read_file","title":"Download, cache and read files — whep_read_file","text":"Used fetch input files needed package's functions built external sources large include directly. public function transparency purposes, users can inspect original inputs package directly processed . requested file exist locally, downloaded public link cached reading . implemented using pins package. supports multiple file formats file versioning.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_read_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download, cache and read files — whep_read_file","text":"","code":"whep_read_file(file_alias, type = \"parquet\", version = NULL)"},{"path":"https://eduaguilera.github.io/whep/reference/whep_read_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download, cache and read files — whep_read_file","text":"file_alias Internal name requested file. can find possible values alias column whep_inputs dataset. type extension file must read. Possible values: parquet: default value code efficiency reasons. csv: Mainly available want human-readable option. parquet version available, useless function already returns dataset R object, origin irrelevant, parquet read faster. Saving file formats transparency accessibility purposes, e.g., share data non-programmers can easily import CSV spreadsheet. likely never set option manually unless reason file supplied e.g. parquet format another one. version version file must read. Possible values: NULL: default value. frozen version chosen make code reproducible. release frozen versions. version string can found whep_inputs version column. \"latest\": overrides frozen version instead fetches latest one available. might might match frozen version. : specific version can also used. details read version column information whep_inputs.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_read_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download, cache and read files — whep_read_file","text":"tibble dataset. information dataset can found code used input processing.","code":""},{"path":"https://eduaguilera.github.io/whep/reference/whep_read_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download, cache and read files — whep_read_file","text":"","code":"whep_read_file(\"read_example\") #> ℹ Fetching files for read_example... #> # A tibble: 1 × 2 #>   col_1                col_2          #>   <chr>                <chr>          #> 1 I'm a sample dataset second version whep_read_file(\"read_example\", type = \"parquet\", version = \"latest\") #> ℹ Fetching files for read_example... #> # A tibble: 1 × 2 #>   col_1                col_2         #>   <chr>                <chr>         #> 1 I'm a sample dataset third version whep_read_file(   \"read_example\",   type = \"csv\",   version = \"20250721T152646Z-ce61b\" ) #> ℹ Fetching files for read_example... #> # A tibble: 1 × 2 #>   col_1                col_2         #>   <chr>                <chr>         #> 1 I'm a sample dataset first version"},{"path":[]},{"path":"https://eduaguilera.github.io/whep/news/index.html","id":"whep-010","dir":"Changelog","previous_headings":"","what":"whep 0.1.0","title":"whep 0.1.0","text":"CRAN release: 2025-07-25 Build supply-use tables (build_supply_use()) (#17). Balance bilateral trade (get_bilateral_trade()) (#8, #9). Create article Follow workflow new contributors (#1, #2, #29). Download large datasets whep_read_file() pins package (#29, #43). Get raw FAOSTAT data get_faostat_data() wrapper (#3). Initial CRAN submission.","code":""}]
